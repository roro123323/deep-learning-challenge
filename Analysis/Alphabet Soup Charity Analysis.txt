Step 4: Writing a Report on the Neural Network Model - Alphabet Soup Charity Analysis Report

Q3- Summary:

Overview:
Alphabet Soup's objective is to identify potential successful applicants for funding using machine learning and neural networks. Our binary classifier predicts the success of applicants based on various features from the provided dataset.

Results:

Data Preprocessing:

Target Variable: "IS_SUCCESSFUL"
Feature Variables: "APPLICATION_TYPE", "AFFILIATION", "CLASSIFICATION", "USE_CASE", "ORGANIZATION", "STATUS", "INCOME_AMT", "SPECIAL_CONSIDERATIONS", "ASK_AMT"
Removed Variables: "EIN" and "NAME"
Compiling, Training, and Evaluating the Model:

Three attempts were made to optimize the model's performance.

1st Try: Two hidden layers, layer 1 with 10 neurons (tanh activation), layer 2 with 20 neurons (sigmoid activation), and an outer layer with 1 unit (sigmoid activation, Adam optimizer).

2nd Try: Three hidden layers, layer 1 with 15 neurons (tanh activation), layer 2 with 25 neurons (sigmoid activation), layer 3 with 25 neurons (relu activation), and an outer layer with 1 unit (sigmoid activation, Adam optimizer).

3rd Try: Three hidden layers, layer 1 with 30 neurons (tanh activation), layer 2 with 25 neurons (sigmoid activation), layer 3 with 20 neurons (sigmoid activation), and an outer layer with 1 unit (sigmoid activation, Adam optimizer).

Best Accuracy Achieved: 0.7286

Recommendation:
To further enhance model performance, exploring other advanced techniques like ensemble methods (Random Forest, Gradient Boosting) or a different neural network architecture, such as a convolutional neural network (CNN) for complex patterns, could be beneficial. Also, hyperparameter tuning might help in achieving better results.

Summary:
The deep learning model demonstrated moderate success with an accuracy of 0.7286. Exploring alternative models and fine-tuning hyperparameters could lead to improved performance in predicting successful applicants for Alphabet Soup's funding.





